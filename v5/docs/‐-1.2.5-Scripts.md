<TABLE width="100%"><TR><TD align="left"><a href="‐-1.2.0-Workflows.md">PREV < 1.2.0 Workflows</a></TD><TD align="right"><a href="‐-1.3.0-Core-Configuration-‐-`core.json`.md">1.3.0 Core Configuration `core.json` > NEXT</a></TD></TR></TABLE>

The workflows are yaml based. Yaml can do a lot but that doesn't mean it's the right tool for the job.

# APPLY_ROOT_TEMPLATE

The `apply_root_template` script provides a clear, structured, and efficient solution for applying a JSON template (`template.json`) to a target JSON file (`data.json`). Below is a comprehensive explanation of the script, including its purpose, workflow, features, and potential improvements.

---

### **Purpose**
The script is designed to:
1. **Merge Templates**: Apply a `template.json` to `data.json` in a controlled way, preserving the integrity of existing data while updating it with new content.
2. **Backup Existing Data**: Before modifying `data.json`, create a backup (`data.json.bak`) for rollback purposes.
3. **Dry-Run Support**: Test changes without modifying the original file, allowing users to validate results before committing them.

---

### **Key Features**

#### 1. **Initialization**
- **Working Directory**: The script ensures it operates in the root directory by dynamically adjusting the working directory:
  ```python
  script_dir = os.path.dirname(os.path.abspath(__file__))
  root_dir = os.path.abspath(os.path.join(script_dir, ".."))
  os.chdir(root_dir)
  ```
- **Logging**: Logs are stored in `workflow.log`, capturing critical details like backups, successful merges, and errors:
  ```python
  logging.basicConfig(filename='workflow.log', level=logging.INFO, format='%(asctime)s %(message)s')
  ```

#### 2. **Dry-Run Mode**
- Controlled via the `DRYRUN` environment variable:
  ```bash
  DRYRUN=true python apply_root_template.py
  ```
  - In `dry-run` mode, changes are saved to a temporary file (`data.json.tmp`) instead of overwriting `data.json`.

#### 3. **Backup Functionality**
- Automatically creates a backup (`data.json.bak`) of the target file before applying changes (if not in `dry-run` mode):
  ```python
  if not DRYRUN and os.path.exists(data_path):
      backup_path = f"{data_path}.bak"
      os.rename(data_path, backup_path)
      logging.info(f"Backup created: {backup_path}")
  ```

#### 4. **Deep Merging**
- The `deep_merge()` function ensures a recursive and schema-safe merge:
  - **Schema Exclusions**: Skips merging fields like `type`, `properties`, and `required`.
  - **Metadata Handling**: Preserves `Metadata` descriptions if present.
  ```python
  def deep_merge(target, source):
      schema_keys = {"type", "properties", "required", "additionalProperties", "patternProperties"}
      for key, value in source.items():
          if key in schema_keys:
              continue
          ...
  ```

#### 5. **Error Handling**
The script handles various error scenarios gracefully:
- **JSONDecodeError**: Ensures invalid JSON is flagged and logged:
  ```python
  except json.JSONDecodeError as e:
      logging.error(f"Error decoding JSON: {e}")
      sys.exit(1)
  ```
- **FileNotFoundError**: Handles missing files and logs the error:
  ```python
  except FileNotFoundError as e:
      logging.error(f"File not found: {e}")
      sys.exit(1)
  ```
- **General Exceptions**: Catches unexpected issues and provides detailed logs for debugging.

#### 6. **Output**
- Saves the merged result to:
  - `data.json` (normal mode).
  - `data.json.tmp` (dry-run mode).
- Logs the status of the operation:
  ```python
  logging.info(f"Template applied {'(DRYRUN)' if DRYRUN else ''} successfully to {output_path}.")
  ```

---

### **How It Works**

1. **Load Files**: Reads `data.json` and `template.json`.
2. **Backup Original Data**: Creates a `.bak` file for `data.json` if modifications are planned.
3. **Merge Template**: Uses `deep_merge()` to recursively integrate the template into the data.
4. **Save Changes**:
   - Writes to `data.json` (normal mode).
   - Writes to `data.json.tmp` (dry-run mode).
5. **Error Handling**: Logs and prints detailed errors if issues occur.

---

### **Usage**

#### Normal Mode:
To apply the template and update `data.json`:
```bash
python apply_root_template.py
```

#### Dry-Run Mode:
To test the changes without modifying `data.json`:
```bash
DRYRUN=true python apply_root_template.py
```

---

### **Potential Enhancements**
1. **Validation**:
   - Add schema validation for `data.json` and `template.json` before merging.
   - Use a JSON schema validation library like `jsonschema`.
2. **Logging Improvements**:
   - Include details of merged keys and changes for better traceability.
3. **Testing**:
   - Create unit tests for the `deep_merge()` function to ensure it handles complex nested structures correctly.
4. **CLI Arguments**:
   - Allow users to specify `data.json` and `template.json` paths as command-line arguments.
   ```bash
   python apply_root_template.py --data=data.json --template=template.json
   ```

---

### **Example Output**

#### **Normal Mode (Successful Merge)**
```bash
Template applied successfully.
```
Log entry:
```
2025-01-25 10:00:00 Backup created: data.json.bak
2025-01-25 10:00:01 Template applied successfully to data.json.
```

#### **Dry-Run Mode**
```bash
Template applied (DRYRUN) successfully.
```
Log entry:
```
2025-01-25 10:05:00 Template applied (DRYRUN) successfully to data.json.tmp.
```

#### **Error Scenario**
```bash
Error decoding JSON: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
```
Log entry:
```
2025-01-25 10:10:00 Error decoding JSON: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
```

---

This script is a well-structured solution for applying templates to JSON data files, ensuring safety, traceability, and flexibility during operations.


# APPLY_TEMPLATE

### **`apply_template` Script Documentation**

The `apply_template` script is a robust tool designed to merge a JSON template (`template.json`) into a target JSON file (`updates/data.json`). It ensures that existing data is preserved while new data from the template is incorporated. This script is particularly useful in managing structured data updates and maintaining a consistent schema across files.

---

### **Purpose**
The script facilitates:
1. **Template Application**: Merging `template.json` into `updates/data.json` with priority for existing data.
2. **Backup and Safety**: Backing up the original `data.json` before applying the template.
3. **Dry-Run Testing**: Allowing safe testing of the merge without modifying the original file.
4. **Error Handling**: Managing JSON parsing errors, file-not-found issues, and unexpected errors gracefully.

---

### **Workflow**

#### **1. Initialization**
- **Directory Setup**: The script sets the working directory to the root of the project:
  ```python
  script_dir = os.path.dirname(os.path.abspath(__file__))
  root_dir = os.path.abspath(os.path.join(script_dir, ".."))
  os.chdir(root_dir)
  ```
- **Logging**: Initializes logging to capture all operations in `workflow.log`:
  ```python
  logging.basicConfig(filename='workflow.log', level=logging.INFO, format='%(asctime)s %(message)s')
  ```

#### **2. Dry-Run Mode**
- **Environment Variable**: The script respects the `DRYRUN` environment variable to determine whether to modify the original file:
  ```bash
  DRYRUN=true python apply_template.py
  ```
- **Output**: Saves changes to `updates/data.json.tmp` in dry-run mode for review:
  ```python
  output_path = f"{data_path}.tmp" if DRYRUN else data_path
  ```

#### **3. Backup Original Data**
- Before making changes, the script backs up the original file as `data.json.bak`:
  ```python
  if not DRYRUN and os.path.exists(data_path):
      backup_path = f"{data_path}.bak"
      os.rename(data_path, backup_path)
      logging.info(f"Backup created: {backup_path}")
  ```

#### **4. Deep Merge Functionality**
The `deep_merge()` function recursively merges the template into the target file:
- **Schema Exclusion**: Skips merging fields like `type`, `properties`, and `required` that pertain to JSON schemas.
- **Metadata Handling**: Ensures metadata descriptions are updated correctly.
  ```python
  def deep_merge(target, source):
      schema_keys = {"type", "properties", "required", "additionalProperties", "patternProperties"}
      for key, value in source.items():
          if key in schema_keys:
              continue
          ...
  ```

#### **5. Template Application**
- **Merge Data**: Combines the contents of `template.json` into `updates/data.json` using `deep_merge()`:
  ```python
  deep_merge(data, template)
  ```
- **Save Changes**: Saves the merged content to `data.json` (normal mode) or `data.json.tmp` (dry-run mode).

#### **6. Error Handling**
The script gracefully handles errors with detailed logging:
- **JSON Parsing Errors**:
  ```python
  except json.JSONDecodeError as e:
      logging.error(f"Error decoding JSON: {e}")
      sys.exit(1)
  ```
- **File Not Found**:
  ```python
  except FileNotFoundError as e:
      logging.error(f"File not found: {e}")
      sys.exit(1)
  ```
- **General Exceptions**:
  ```python
  except Exception as e:
      logging.error(f"Unexpected error: {e}")
      sys.exit(1)
  ```

---

### **Usage**

#### **Normal Mode**
To apply the template and update `updates/data.json`:
```bash
python apply_template.py
```

#### **Dry-Run Mode**
To test the changes without modifying `updates/data.json`:
```bash
DRYRUN=true python apply_template.py
```

---

### **Key Features**
1. **Backup and Safety**: Ensures the integrity of the original data by creating a backup before modifications.
2. **Dry-Run Mode**: Allows users to preview changes without making permanent modifications.
3. **Recursive Merging**: Merges nested dictionaries while preserving schema integrity.
4. **Error Resilience**: Logs detailed errors and exits gracefully in case of issues.

---

### **Example Output**

#### **Normal Mode (Success)**
```bash
Template applied successfully.
```
**Log Entry**:
```
2025-01-25 10:00:00 Backup created: updates/data.json.bak
2025-01-25 10:00:01 Template applied successfully to updates/data.json.
```

#### **Dry-Run Mode**
```bash
Template applied (DRYRUN) successfully.
```
**Log Entry**:
```
2025-01-25 10:05:00 Template applied (DRYRUN) successfully to updates/data.json.tmp.
```

#### **Error Scenario**
```bash
Error decoding JSON: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
```
**Log Entry**:
```
2025-01-25 10:10:00 Error decoding JSON: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
```

---

### **Potential Enhancements**
1. **Schema Validation**: Add support for validating `data.json` and `template.json` using a JSON schema validator.
2. **Command-Line Arguments**: Allow users to specify the target and template files via CLI options:
   ```bash
   python apply_template.py --data updates/data.json --template template.json
   ```
3. **Enhanced Logging**: Include detailed reports of the changes made during the merge.
4. **Unit Tests**: Write test cases for `deep_merge()` to validate its behavior with complex nested dictionaries.

---

The `apply_template` script is a versatile and safe tool for updating JSON data files using templates. It ensures data integrity, supports testing through dry-run mode, and provides robust error handling for reliable operation.


# CLEANUP_PURGE

### **`cleanup_purge` Script Documentation**

The `cleanup_purge` script is designed to manage file storage by automatically removing old files based on a defined retention period and clearing temporary files from specific directories. This script ensures that Project MyShelf remains organized and free from unnecessary clutter.

---

### **Purpose**
1. **Archive Maintenance**: Deletes files older than a specified number of days from the archive directory.
2. **Updates Cleanup**: Removes the `updates/data.json` file to reset the updates folder for new operations.
3. **Dry-Run Testing**: Allows testing cleanup without making actual changes.
4. **Environment Configurations**: Customizable using environment variables for retention period and operation mode.

---

### **Workflow**

#### **1. Initialization**
- **Environment Variables**:
  - `DRYRUN`: Enables a mode where no files are deleted, and actions are only simulated.
    ```bash
    DRYRUN=true python cleanup_purge.py
    ```
  - `CLEANUP_PERIOD`: Specifies the retention period (default: 10 days).
    ```bash
    CLEANUP_PERIOD=30 python cleanup_purge.py
    ```

- **Directory Setup**: Configures paths for the archive and updates folder:
  ```python
  ARCHIVE_DIR = "../archive"
  UPDATES_DATA_JSON = "../updates/data.json"
  ```

#### **2. File Date Extraction**
The script extracts dates to determine file age:
- **Filename Patterns**:
  - Matches dates in formats like `YYYY-MM-DD` or `YYYYMMDD`:
    ```python
    date_patterns = [r"(\d{4}-\d{2}-\d{2})", r"(\d{8})"]
    ```
- **Fallback to File Metadata**: If no date is found in the filename, the script falls back to the file's creation date:
    ```python
    return datetime.fromtimestamp(os.path.getmtime(file_path))
    ```

#### **3. Cleanup Logic**
- **Archive Cleanup**: Deletes files older than the specified retention period:
  ```python
  def cleanup_old_files(directory, days):
      cutoff_date = datetime.now() - timedelta(days=days)
      ...
      if file_creation_date < cutoff_date:
          os.remove(file_path)
  ```
- **Updates Cleanup**: Removes `updates/data.json` if it exists:
  ```python
  def remove_updates_data_json():
      if os.path.exists(UPDATES_DATA_JSON):
          os.remove(UPDATES_DATA_JSON)
  ```

#### **4. Dry-Run Mode**
Simulates cleanup without actual file deletion:
```python
if DRYRUN:
    print("Dry Run mode enabled. No changes will be made.")
    return
```

#### **5. Logging and Output**
The script prints and logs removed files:
```python
print("Removed:", file_path)
```

---

### **Usage**

#### **Standard Execution**
To remove files older than the default retention period (10 days):
```bash
python cleanup_purge.py
```

#### **Custom Cleanup Period**
To set a specific retention period (e.g., 30 days):
```bash
CLEANUP_PERIOD=30 python cleanup_purge.py
```

#### **Dry-Run Mode**
To test cleanup actions without deleting any files:
```bash
DRYRUN=true python cleanup_purge.py
```

---

### **Key Features**
1. **Date-Based Deletion**: Uses filenames or file metadata to determine file age.
2. **Retention Period**: Customizable via `CLEANUP_PERIOD` environment variable.
3. **Dry-Run Mode**: Allows users to preview cleanup actions safely.
4. **Comprehensive Cleanup**: Targets both archive files and `updates/data.json`.

---

### **Example Outputs**

#### **Dry-Run Mode**
```bash
Dry Run mode enabled. No changes will be made.
```

#### **Files Removed**
```bash
Removed: ../archive/data.json.2025-01-15
Removed: ../updates/data.json
Files removed: ['../archive/data.json.2025-01-15', '../updates/data.json']
```

#### **No Files to Remove**
```bash
No files to remove.
```

---

### **Potential Enhancements**
1. **Logging**: Add logging for removed files to a dedicated log file for better traceability.
2. **Report Generation**: Generate a summary report of deleted files, including file names and removal dates.
3. **Command-Line Arguments**: Support for passing custom paths and settings directly via CLI.
4. **Extended Date Formats**: Add support for additional date formats in filenames.

---

The `cleanup_purge` script ensures a clean and organized file system for Project MyShelf, with flexible options for testing and configuration. Its dry-run mode and error-safe operations make it a reliable tool for archive and updates management.


# GENERATE_CORE_GRAPH

### **`generate_core_graph` Script Documentation**

The `generate_core_graph` script is designed to construct a comprehensive directed graph representing the content structure and relationships within the Project MyShelf repository. The script processes files and directories, extracts metadata and keywords, and saves the graph in a JSON-compatible format.

---

### **Purpose**
1. **Core Graph Generation**: Build a detailed representation of MyLibrary and context directories.
2. **Keyword Extraction**: Enhance nodes with relevant keywords for improved search and navigation.
3. **Backup and Restore**: Automatically backup existing core graphs before regenerating.
4. **Context-Specific Processing**: Tailor graph traversal for specific file types, such as session context files.

---

### **Workflow**

#### **1. Initialization**
- **Environment Variables**:
  - `KEYWORD_GOAL`: Target number of keywords per file. Defaults to `10`.
  - `KEYWORD_ATTEMPT_THRESHOLD`: Maximum attempts to extract keywords. Defaults to `10`.
  ```bash
  KEYWORD_GOAL=15 KEYWORD_ATTEMPT_THRESHOLD=5 python generate_core_graph.py
  ```

- **Paths and Backup**:
  - `CORE_GRAPH_PATH`: Location to save the generated core graph (`snapshots/core.graph.json`).
  - `BACKUP_PATH`: Backup location for the previous graph (`snapshots/core.graph.json.old`).

#### **2. File Metadata and Keywords**
- Extract metadata for each file:
  - File size
  - Relative path
  - Extracted keywords
  ```python
  metadata = {
      "type": "file",
      "size": os.path.getsize(file_path),
      "path": relative_file_path,
      "keywords": extract_keywords_from_file(file_path)
  }
  ```
- **Keyword Extraction**:
  - Uses **spaCy** and **TF-IDF Vectorizer** for keyword extraction.
  - Removes ignored keywords defined in `IGNORE_KEYWORDS`.

#### **3. Directory Traversal**
- **Allowed Paths**:
  - `MyLibrary/`
  - `context/`
- **Context Handling**:
  - Processes only files with the prefix `context.session.*` in the `context/` directory.
  ```python
  if "context" in start_path:
      if entry.name.startswith("context.session."):
          graph.add_node(relative_entry_path, **metadata)
          graph.add_edge(relative_path(start_path, repo_root), relative_entry_path)
  ```
- **Graph Construction**:
  - Nodes represent files and directories.
  - Edges represent relationships between directories and their contents.

#### **4. Graph Construction**
- Initializes a directed graph (`networkx.DiGraph`).
- Adds nodes and edges dynamically during traversal.
```python
graph.add_node(relative_entry_path, **metadata)
graph.add_edge(relative_path(start_path, repo_root), relative_entry_path)
```

#### **5. Backup and Save**
- Backs up existing graph:
  ```python
  shutil.copy(CORE_GRAPH_PATH, BACKUP_PATH)
  ```
- Saves the new graph in JSON format using `json_graph.node_link_data`:
  ```python
  graph_data = json_graph.node_link_data(graph)
  with open(CORE_GRAPH_PATH, "w", encoding="utf-8") as f:
      json.dump(graph_data, f, indent=4)
  ```

---

### **Features**

1. **Dynamic Keyword Extraction**:
   - Leverages **spaCy** for Named Entity Recognition.
   - Uses **TF-IDF Vectorizer** to prioritize important terms.

2. **Path Validation**:
   - Ensures traversal is restricted to allowed paths (`MyLibrary` and `context`).

3. **Context-Aware Processing**:
   - Processes session files in the `context/` directory differently from other files.

4. **Error Handling**:
   - Skips hidden files and directories.
   - Logs errors encountered during keyword extraction or traversal.

5. **Graph Details**:
   - Nodes represent files and directories with associated metadata.
   - Edges represent relationships between parent directories and their contents.

---

### **Execution**

#### **Standard Execution**
To generate the core graph:
```bash
python generate_core_graph.py
```

#### **Environment Configuration**
Set keyword extraction parameters:
```bash
KEYWORD_GOAL=15 KEYWORD_ATTEMPT_THRESHOLD=5 python generate_core_graph.py
```

---

### **Outputs**

1. **Core Graph**:
   - Saved to `snapshots/core.graph.json`.
   - Includes nodes with metadata and keywords, and edges representing relationships.

2. **Backup**:
   - Previous graph saved to `snapshots/core.graph.json.old`.

3. **Logs**:
   - Outputs information about added nodes, edges, and any errors encountered.

---

### **Error Handling**

- **Keyword Extraction Failure**:
  Logs the error and skips the affected file:
  ```python
  print(f"Keyword extraction error for {file_path}: {e}")
  ```

- **Traversal Errors**:
  Logs errors encountered during directory traversal:
  ```python
  print(f"Error processing {start_path}: {e}")
  ```

---

### **Enhancements**
1. **Parallel Processing**: Speed up traversal and keyword extraction with multi-threading.
2. **Extended Metadata**: Include additional metadata, such as file creation or modification dates.
3. **Configurable Ignore List**: Allow users to modify the `IGNORE_KEYWORDS` list dynamically.

The `generate_core_graph` script provides a robust and extensible framework for building a graph-based representation of MyLibrary, enabling seamless navigation and enhanced content discovery.


# GENERATE_MINI_GRAPHS

### **`mini_graph` Script Documentation**

The `mini_graph` script is designed to create modular, localized subgraphs (mini-graphs) from a comprehensive core graph. These mini-graphs allow for efficient navigation and targeted exploration of specific sections of MyLibrary, enhancing performance and scalability.

---

### **Purpose**

1. **Modular Graphs**: Generate subgraphs from the core graph, focusing on specific directories or sections.
2. **Efficient Navigation**: Facilitate quicker traversal by isolating relevant parts of the graph.
3. **Scalable Architecture**: Enable localized graph-based operations without processing the entire core graph.

---

### **Workflow**

#### **1. Load Core Graph**
- Loads the core graph from the predefined `CORE_GRAPH_PATH` (`snapshots/core.graph.json`) using `networkx` and `json_graph`.
- Core graph serves as the global representation of all nodes and relationships.
```python
def load_core_graph():
    with open(CORE_GRAPH_PATH, "r", encoding="utf-8") as f:
        return json_graph.node_link_graph(json.load(f))
```

#### **2. Generate Mini-Graphs**
The script traverses the core graph to extract subgraphs for each directory:
1. **Identify Directories**:
   - Nodes with `type: directory` are used as starting points for subgraph generation.

2. **Extract Subgraph**:
   - Includes the directory node and all its descendants using `nx.descendants`.

3. **Normalize Keywords**:
   - Consolidates keywords for all nodes in the subgraph, ensuring lowercase uniqueness.

4. **Save Mini-Graphs**:
   - Subgraphs are saved in `snapshots/mini-graphs/` with filenames based on their path.
   - Example: `MyLibrary/647` → `snapshots/mini-graphs/MyLibrary_647.graph.json`.

5. **Track Processed Nodes**:
   - Avoids duplicate subgraph generation by maintaining a set of processed directories.

```python
def generate_mini_graphs():
    os.makedirs(MINI_GRAPHS_DIR, exist_ok=True)
    core_graph = load_core_graph()
    processed_subgraphs = set()
    for node, data in core_graph.nodes(data=True):
        if data.get("type") == "directory" and node not in processed_subgraphs:
            subgraph = core_graph.subgraph(nx.descendants(core_graph, node) | {node})
            mini_graph_path = os.path.join(
                MINI_GRAPHS_DIR, node.replace("/", "_") + ".graph.json"
            )
            mini_graph_data = json_graph.node_link_data(subgraph)
            for subnode in mini_graph_data["nodes"]:
                subnode["keywords"] = list(
                    {k.lower() for k in subnode.get("keywords", [])}
                )
            with open(mini_graph_path, "w", encoding="utf-8") as f:
                json.dump(mini_graph_data, f, indent=4)
            processed_subgraphs.add(node)
            print(f"Mini-graph created for: {node}")
```

---

### **Key Features**

1. **Dynamic Subgraph Extraction**:
   - Automatically isolates nodes relevant to each directory for localized exploration.

2. **Keyword Normalization**:
   - Deduplicates and standardizes keywords for consistency across mini-graphs.

3. **Incremental Graph Creation**:
   - Avoids duplicate processing with the `processed_subgraphs` set.

4. **Flexible Storage**:
   - Saves each mini-graph in the `snapshots/mini-graphs` directory with a structured naming convention.

---

### **Outputs**

- **Mini-Graph Files**:
  - Each mini-graph represents a directory and its contents as a self-contained JSON graph.
  - Example structure:
    ```
    {
      "nodes": [
        {
          "id": "MyLibrary/647",
          "type": "directory",
          "keywords": ["bars", "cocktails", "inventory"]
        },
        {
          "id": "MyLibrary/647/mybar.json",
          "type": "file",
          "size": 1240,
          "keywords": ["bar", "cocktails", "inventory"]
        }
      ],
      "links": [
        {
          "source": "MyLibrary/647",
          "target": "MyLibrary/647/mybar.json"
        }
      ]
    }
    ```
- **Directory**:
  - `snapshots/mini-graphs/`
  - Example filenames:
    - `MyLibrary_647.graph.json`
    - `MyLibrary_641.graph.json`

---

### **Execution**

#### **Standard Execution**
Run the script to generate mini-graphs:
```bash
python mini_graph.py
```

#### **Graph Prerequisite**
Ensure the core graph (`snapshots/core.graph.json`) exists and is up-to-date before running.

---

### **Error Handling**

1. **Missing Core Graph**:
   - If the core graph is not found, the script will raise a `FileNotFoundError`.

2. **Keyword Processing Errors**:
   - Logs and skips nodes with keyword extraction issues.

3. **Directory Traversal**:
   - Skips hidden files and directories.

---

### **Enhancements**
1. **Graph Validation**:
   - Add validation to ensure mini-graphs conform to schema requirements.
2. **Parallel Processing**:
   - Speed up graph generation with multi-threaded or asynchronous processing.
3. **Custom Directory Filters**:
   - Allow users to specify directories or nodes for which mini-graphs should be generated.

The `mini_graph` script provides a structured and efficient way to break down a large graph into manageable components, empowering users to perform localized operations and navigate content seamlessly.


# GENERATE_GRAPH_REGISTRY

### **`graph_registry` Script Documentation**

The `graph_registry` script automates the creation of mini-graphs derived from the `core.graph.json`. This enables modular graph exploration and optimized graph-based operations by breaking the core graph into localized subgraphs for specific nodes.

---

### **Purpose**

1. **Mini-Graph Generation**:
   - Extract and save subgraphs starting from specific nodes in the core graph.

2. **Enhanced Navigation**:
   - Facilitate localized exploration by linking mini-graphs with their parent nodes.

3. **Scalability**:
   - Reduce computational overhead by isolating relevant sections of the graph.

---

### **Workflow**

#### **1. Load Core Graph**
The script begins by loading the core graph (`snapshots/core.graph.json`) using `networkx` and `json_graph`.
- **Path**: `CORE_GRAPH_PATH`
- **Purpose**: Centralized representation of all nodes and relationships.

```python
def load_core_graph():
    """Load the core graph data from the JSON file."""
    if not os.path.exists(CORE_GRAPH_PATH):
        raise FileNotFoundError(f"Core graph not found at {CORE_GRAPH_PATH}")

    with open(CORE_GRAPH_PATH, "r", encoding="utf-8") as f:
        return json_graph.node_link_graph(json.load(f))
```

---

#### **2. Extract Subgraph**
Each node in the core graph is processed individually:
- **Node Identification**: Traverse the core graph using `nx.descendants`.
- **Subgraph Creation**: Extract a subgraph containing the node and all its descendants.

```python
def extract_subgraph(core_graph, node_id):
    """Extract a subgraph starting from a specific node ID."""
    if node_id not in core_graph:
        print(f"Node {node_id} not found in the core graph. Skipping.")
        return None

    subgraph_nodes = nx.descendants(core_graph, node_id) | {node_id}
    return core_graph.subgraph(subgraph_nodes).copy()
```

---

#### **3. Normalize and Save Mini-Graphs**
- **Normalize Keywords**:
  - Deduplicate and convert all keywords to lowercase for consistency.
- **Save Mini-Graphs**:
  - Mini-graphs are saved in `snapshots/mini-graphs/` with filenames based on the node ID.

```python
def save_subgraph(subgraph, graph_id):
    """Save a subgraph to a file."""
    subgraph_path = os.path.join(MINI_GRAPHS_DIR, f"{graph_id.replace('/', '_')}.graph.json")
    os.makedirs(os.path.dirname(subgraph_path), exist_ok=True)

    # Prepare node-link data for JSON serialization
    subgraph_data = json_graph.node_link_data(subgraph)

    # Save the subgraph data to the file
    with open(subgraph_path, "w", encoding="utf-8") as f:
        json.dump(subgraph_data, f, indent=4)
    print(f"Mini-graph saved to {subgraph_path}")
```

---

#### **4. Process Each Node**
Each node in the core graph is iteratively processed to generate its corresponding mini-graph:
1. Extract the subgraph for the node.
2. Normalize keywords for all nodes in the subgraph.
3. Save the mini-graph to the `snapshots/mini-graphs/` directory.

```python
def process_node(core_graph, node_id):
    """Process a single node to create its mini-graph."""
    subgraph = extract_subgraph(core_graph, node_id)
    if subgraph:
        # Normalize keywords to lowercase
        for node in subgraph.nodes(data=True):
            if "keywords" in node[1]:
                node[1]["keywords"] = list(set(kw.lower() for kw in node[1]["keywords"]))
        save_subgraph(subgraph, node_id)
```

---

#### **5. Generate Mini-Graphs**
The core graph is loaded, and all nodes are processed to generate mini-graphs.

```python
def generate_mini_graphs():
    """Generate mini-graphs for each node in the core graph."""
    core_graph = load_core_graph()
    os.makedirs(MINI_GRAPHS_DIR, exist_ok=True)

    for node_id in core_graph.nodes:
        print(f"Processing node: {node_id}")
        process_node(core_graph, node_id)
```

---

### **Execution**

#### **Standard Execution**
Run the script to generate mini-graphs for all nodes in the core graph:
```bash
python graph_registry.py
```

#### **Prerequisites**
- Ensure `core.graph.json` exists and is up-to-date in `snapshots/`.

---

### **Outputs**

1. **Mini-Graph Files**:
   - Each mini-graph represents a subgraph starting from a node.
   - Files are stored in `snapshots/mini-graphs/` with structured filenames:
     - Example: `MyLibrary_647.graph.json`.

2. **File Structure**:
   ```
   snapshots/
   ├── core.graph.json
   └── mini-graphs/
       ├── MyLibrary_647.graph.json
       ├── MyLibrary_641.graph.json
       └── ...
   ```

---

### **Error Handling**

1. **Core Graph Missing**:
   - If `core.graph.json` is not found, a `FileNotFoundError` is raised.

2. **Node Not Found**:
   - Skips nodes missing in the core graph and logs a warning.

3. **Keyword Normalization**:
   - Handles missing or invalid keywords gracefully, ensuring data consistency.

---

### **Enhancements**

1. **Selective Processing**:
   - Allow users to specify which nodes to process.
2. **Parallel Processing**:
   - Speed up mini-graph generation using multi-threading.
3. **Graph Validation**:
   - Validate the structure and metadata of each mini-graph after creation.

The `graph_registry` script simplifies graph-based operations by creating modular mini-graphs for targeted analysis, navigation, and scalability.


# GENERATE_CACHE_GRAPH

### **`cache_graph` Script Documentation**

The `cache_graph` script dynamically creates a thematic registry (`theme.registry.json`) by extracting themes and keywords from context files, core graphs, and mini-graphs. The generated registry maps relevant themes to associated mini-graphs with calculated weights for optimized traversal and exploration.

---

### **Purpose**

1. **Thematic Mapping**:
   - Build a registry linking context themes to mini-graphs based on keyword relevance.

2. **Performance Optimization**:
   - Pre-calculate weights for theme-graph relationships to enhance query performance.

3. **Dynamic Exploration**:
   - Dynamically align themes extracted from session context files to relevant sections of MyShelf.

---

### **Workflow**

#### **1. Extract Keywords from Context Files**
The script loads session context files (`context.session.*.md`) and extracts themes using SpaCy for Natural Language Processing (NLP):
- **Path**: `context/`
- **Output**: A dictionary mapping themes to context files.

```python
def load_context_files(context_dir):
    """Load context session files and extract themes."""
    themes = defaultdict(list)
    for root, _, files in os.walk(context_dir):
        for file in files:
            if file.startswith("context.session.") and file.endswith(".md"):
                file_path = os.path.join(root, file)
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        content = f.read()
                    keywords = extract_keywords_with_spacy(content) - IGNORE_KEYWORDS
                    for keyword in keywords:
                        themes[keyword].append(file_path)
                except UnicodeDecodeError:
                    print(f"Skipping non-UTF-8 file: {file_path}")
    return themes
```

---

#### **2. Load Keywords from Core Graph**
Extract all keywords from the core graph (`core.graph.json`) to ensure alignment between context themes and MyShelf's global structure.

```python
def load_keywords_from_core(core_path):
    """Extract keywords from core.graph.json."""
    with open(core_path, "r", encoding="utf-8") as f:
        core_graph = json.load(f)

    keywords = set()
    for node in core_graph.get("nodes", []):
        node_keywords = node.get("keywords", [])
        keywords.update(k.lower() for k in node_keywords)
    return keywords
```

---

#### **3. Match Themes to Mini-Graphs**
Each theme extracted from context files is compared with keywords in mini-graphs (`snapshots/mini-graphs/`). The script calculates a weight for each theme-mini-graph pair:
- **Weight Calculation**:
  - Match Count: Number of overlapping keywords.
  - Total Keywords: Total keywords in the mini-graph.
  - Weight = `Match Count / Total Keywords`.

```python
def match_themes_to_mini_graphs(themes, mini_graph_dir):
    """Match themes to mini-graphs and calculate weight."""
    theme_to_graphs = defaultdict(dict)
    for mini_graph in os.listdir(mini_graph_dir):
        if mini_graph.endswith(".graph.json") and not mini_graph.startswith("context"):
            mini_graph_path = os.path.join(mini_graph_dir, mini_graph)
            with open(mini_graph_path, "r", encoding="utf-8") as f:
                graph_data = json.load(f)
                mini_graph_keywords = {
                    keyword.lower() for node in graph_data.get("nodes", []) for keyword in node.get("keywords", [])
                }
                for theme, files in themes.items():
                    match_count = len(mini_graph_keywords & {theme})
                    total_keywords = len(mini_graph_keywords) or 1
                    weight = match_count / total_keywords
                    if weight >= THEME_MINI_GRAPH_WEIGHT_THRESHOLD:
                        theme_to_graphs[theme][mini_graph] = round(weight, 2)
    return theme_to_graphs
```

---

#### **4. Generate Theme Registry**
The script creates `theme.registry.json`, which maps themes to:
1. **Context Files**: Files contributing to the theme.
2. **Mini-Graphs**: Relevant graphs with associated weights.

```python
def generate_theme_registry(themes, theme_to_graphs, registry_path):
    """Generate the theme registry mapping themes to mini-graphs."""
    registry = {}
    for theme, files in themes.items():
        mini_graphs = [
            {"name": graph, "weight": weight}
            for graph, weight in sorted(theme_to_graphs.get(theme, {}).items(), key=lambda x: -x[1])
            if weight > 0.0
        ]
        if mini_graphs:  # Only add themes with meaningful relationships
            registry[theme] = {
                "files": files,
                "mini-graphs": mini_graphs,
            }
    with open(registry_path, "w", encoding="utf-8") as f:
        json.dump(registry, f, indent=4)
    print(f"Saved theme registry to {registry_path}")
```

---

#### **5. Clean Temporary Files**
Remove temporary `.theme.graph.json` files to ensure a clean workspace.

```python
def remove_temporary_files():
    """Remove temporary *.theme.graph.json files."""
    for root, _, files in os.walk(THEME_GRAPH_DIR):
        for file in files:
            if file.endswith(".theme.graph.json"):
                os.remove(os.path.join(root, file))
                print(f"Removed temporary file: {file}")
```

---

#### **6. Create Index Copy**
Copy `theme.registry.json` to `index.json` for compatibility with MyShelf's navigation system.

```python
def create_index_copy():
    """Copy theme.registry.json to index.json."""
    if os.path.exists(THEME_REGISTRY_PATH):
        shutil.copyfile(THEME_REGISTRY_PATH, INDEX_PATH)
        print(f"Copied theme registry to index.json: {INDEX_PATH}")
```

---

### **Execution**

#### **Standard Execution**
Run the script to generate the thematic registry:
```bash
python cache_graph.py
```

#### **Prerequisites**
1. Ensure the following files/directories exist:
   - `snapshots/core.graph.json`
   - `snapshots/mini-graphs/`
   - `context/`
2. Install SpaCy and necessary models:
   ```bash
   pip install spacy
   python -m spacy download en_core_web_sm
   ```

---

### **Outputs**

1. **Theme Registry**:
   - Path: `snapshots/mini-graph-themes/theme.registry.json`.
   - Structure:
     ```json
     {
         "theme1": {
             "files": ["context.session.001.20250101.md"],
             "mini-graphs": [
                 {"name": "graph1.graph.json", "weight": 0.75}
             ]
         }
     }
     ```

2. **Index Copy**:
   - Path: `snapshots/mini-graph-themes/index.json`.

---

### **Error Handling**

1. **Empty Registry**:
   - If no themes match any mini-graphs, the script raises an error.
2. **Non-UTF-8 Files**:
   - Skips non-UTF-8 context files with a warning.
3. **Missing Core Graph**:
   - Raises `FileNotFoundError` if `core.graph.json` is unavailable.

---

### **Enhancements**

1. **Parallel Processing**:
   - Process context files and mini-graphs concurrently to improve performance.
2. **Thematic Weight Threshold**:
   - Allow dynamic adjustment of `THEME_MINI_GRAPH_WEIGHT_THRESHOLD`.

The `cache_graph` script ensures efficient thematic exploration, bridging session context with MyShelf's modular graph structure.


# VALIDATE_JSON

### **`Validate JSON` Script Documentation**

The `validate_json` script is designed to ensure that JSON files are both syntactically correct and structurally compliant with a predefined JSON schema. It logs all operations and provides immediate feedback on validation results.

---

### **Purpose**

1. **Syntax Validation**:
   - Checks whether a JSON file is well-formed and free of syntax errors.

2. **Schema Validation**:
   - Ensures that a JSON file adheres to a specified structure as defined by a schema.

3. **Logging**:
   - Logs all validation results for debugging and traceability.

---

### **Workflow**

#### **1. Syntax Validation**
The script first checks if the JSON file has valid syntax. It loads the file and attempts to parse it. Any errors are logged and printed.

```python
def validate_json_syntax(file_path):
    """Validate JSON file syntax."""
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)
        logging.info(f"Syntax validation passed for {file_path}.")
        print(f"Syntax validation passed for {file_path}.")
        return data
    except json.JSONDecodeError as e:
        logging.error(f"JSON syntax validation failed for {file_path}: {e}")
        print(f"JSON syntax validation failed for {file_path}: {e}")
        return None
    except FileNotFoundError as e:
        logging.error(f"File not found: {file_path}")
        print(f"File not found: {file_path}")
        return None
```

- **Input**: Path to the JSON file.
- **Output**:
  - Returns the loaded JSON object if syntax validation passes.
  - Returns `None` if validation fails.

---

#### **2. Structure Validation**
After syntax validation, the script checks whether the JSON file adheres to a specific schema using the `jsonschema` library.

```python
def validate_json_structure(data, schema_path):
    """Validate JSON data against a schema."""
    try:
        with open(schema_path, 'r') as f:
            schema = json.load(f)
        validate(instance=data, schema=schema)
        logging.info("Structure validation passed against schema.")
        print("Structure validation passed against schema.")
        return True
    except ValidationError as e:
        logging.error(f"Schema validation failed: {e}")
        print(f"Schema validation failed: {e}")
        return False
    except FileNotFoundError as e:
        logging.error(f"Schema file not found: {schema_path}")
        print(f"Schema file not found: {schema_path}")
        return False
```

- **Input**:
  - JSON object from syntax validation.
  - Path to the schema file.
- **Output**:
  - Returns `True` if the JSON file matches the schema.
  - Returns `False` if validation fails or the schema file is missing.

---

### **Execution**

#### **Command-Line Execution**
Run the script using Python:
```bash
python validate_json.py
```

#### **Environment Setup**
1. Ensure the following files are in the root directory:
   - **`updates/data.json`**: JSON file to be validated.
   - **`schema.json`**: JSON schema against which `data.json` will be validated.

2. Install dependencies:
   ```bash
   pip install jsonschema
   ```

---

### **Logging**

- **Log File**: `workflow.log`
- **Information Logged**:
  - Successful syntax validation.
  - Successful structure validation.
  - Errors encountered during syntax or schema validation.

Example Log Entry:
```
2025-01-25 12:00:00 Syntax validation passed for updates/data.json.
2025-01-25 12:00:01 Structure validation passed against schema.
```

---

### **Error Handling**

1. **Syntax Errors**:
   - Logs and prints the error message.
   - Terminates the script with `sys.exit(1)`.

2. **Schema Validation Errors**:
   - Logs and prints the validation error details.
   - Terminates the script with `sys.exit(1)`.

3. **Missing Files**:
   - Logs and prints an error message if the JSON file or schema file is not found.

---

### **Modifications for Custom Use Cases**

1. **Custom JSON and Schema Paths**:
   - Update the `data_file` and `schema_file` variables with the desired paths.

2. **Custom Logging**:
   - Modify `logging.basicConfig` to change the log file path or format.

---

### **Example Usage**

1. **Validating a File**:
   - Place `updates/data.json` and `schema.json` in the root directory.
   - Run the script:
     ```bash
     python validate_json.py
     ```
   - Expected Output (if successful):
     ```
     Syntax validation passed for updates/data.json.
     Structure validation passed against schema.
     ```

2. **Error Scenarios**:
   - Missing or invalid JSON:
     ```
     JSON syntax validation failed for updates/data.json: Expecting ',' delimiter: line 10 column 15 (char 115).
     ```
   - Schema mismatch:
     ```
     Schema validation failed: 'title' is a required property.
     ```

This script is a robust tool for ensuring the integrity of JSON files in your workflows.


# VALIDATE_ROOT_JSON

### **`Validate Root JSON` Script Documentation**

The `validate_root_json` script is designed to ensure that the root-level `data.json` file is both syntactically valid and adheres to a specified schema (`schema.json`). This script plays a critical role in maintaining data integrity and consistency in workflows.

---

### **Purpose**

1. **Syntax Validation**:
   - Confirms that `data.json` is well-formed and contains no syntax errors.

2. **Schema Validation**:
   - Ensures that `data.json` complies with a predefined structure specified in `schema.json`.

3. **Logging**:
   - Records validation results and errors for debugging and audit purposes.

---

### **Workflow**

#### **1. Syntax Validation**
The script begins by checking the syntax of `data.json`. It attempts to load the file and logs the results.

```python
def validate_json_syntax(file_path):
    """Validate JSON file syntax."""
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)
        logging.info(f"Syntax validation passed for {file_path}.")
        print(f"Syntax validation passed for {file_path}.")
        return data
    except json.JSONDecodeError as e:
        logging.error(f"JSON syntax validation failed for {file_path}: {e}")
        print(f"JSON syntax validation failed for {file_path}: {e}")
        return None
    except FileNotFoundError as e:
        logging.error(f"File not found: {file_path}")
        print(f"File not found: {file_path}")
        return None
```

- **Input**: Path to `data.json`.
- **Output**:
  - Returns the loaded JSON object if validation passes.
  - Logs and prints errors if validation fails.

---

#### **2. Schema Validation**
If syntax validation succeeds, the script validates the JSON data against `schema.json` using the `jsonschema` library.

```python
def validate_json_structure(data, schema_path):
    """Validate JSON data against a schema."""
    try:
        with open(schema_path, 'r') as f:
            schema = json.load(f)
        validate(instance=data, schema=schema)
        logging.info("Structure validation passed against schema.")
        print("Structure validation passed against schema.")
        return True
    except ValidationError as e:
        logging.error(f"Schema validation failed: {e}")
        print(f"Schema validation failed: {e}")
        return False
    except FileNotFoundError as e:
        logging.error(f"Schema file not found: {schema_path}")
        print(f"Schema file not found: {schema_path}")
        return False
```

- **Input**:
  - JSON data from syntax validation.
  - Path to `schema.json`.
- **Output**:
  - Returns `True` if the JSON data adheres to the schema.
  - Logs and prints validation errors if the data fails.

---

### **Execution**

#### **Command-Line Execution**
Run the script using Python:
```bash
python validate_root_json.py
```

#### **Environment Setup**
1. Ensure the following files are in the root directory:
   - `data.json`: The file to validate.
   - `schema.json`: The schema for validation.

2. Install dependencies:
   ```bash
   pip install jsonschema
   ```

---

### **Logging**

- **Log File**: `workflow.log`
- **Details Logged**:
  - Success or failure of syntax validation.
  - Success or failure of schema validation.
  - Error details in case of failures.

Example Log Entry:
```
2025-01-25 12:00:00 Syntax validation passed for data.json.
2025-01-25 12:00:01 Structure validation passed against schema.
```

---

### **Error Handling**

1. **Syntax Errors**:
   - Logs and prints an error message with details.
   - Terminates the script with `sys.exit(1)`.

2. **Schema Validation Errors**:
   - Logs and prints a detailed error message.
   - Terminates the script with `sys.exit(1)`.

3. **File Not Found**:
   - Logs and prints a message indicating that `data.json` or `schema.json` is missing.
   - Terminates the script with `sys.exit(1)`.

---

### **Example Scenarios**

#### **Successful Validation**
- Command:
  ```bash
  python validate_root_json.py
  ```
- Output:
  ```
  Syntax validation passed for data.json.
  Structure validation passed against schema.
  ```

#### **Syntax Error**
- Scenario: `data.json` contains invalid syntax.
- Output:
  ```
  JSON syntax validation failed for data.json: Expecting ',' delimiter: line 10 column 15 (char 115).
  ```

#### **Schema Validation Error**
- Scenario: `data.json` is missing a required field.
- Output:
  ```
  Schema validation failed: 'title' is a required property.
  ```

#### **Missing File**
- Scenario: `schema.json` is not found.
- Output:
  ```
  File not found: schema.json
  ```

---

### **Modifications**

1. **Custom Paths**:
   - Update the `data_file` and `schema_file` variables to validate files in different locations.

2. **Custom Logging**:
   - Modify `logging.basicConfig` to change the log file path or format.

---

This script ensures data integrity for `data.json` by validating its syntax and structure, making it an essential component for robust workflows.




<TABLE width="100%"><TR><TD align="left"><a href="‐-1.2.0-Workflows.md">PREV < 1.2.0 Workflows</a></TD><TD align="right"><a href="‐-1.3.0-Core-Configuration-‐-`core.json`.md">1.3.0 Core Configuration `core.json` > NEXT</a></TD></TR></TABLE>
